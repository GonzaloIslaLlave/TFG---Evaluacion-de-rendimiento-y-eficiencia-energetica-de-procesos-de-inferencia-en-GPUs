$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
make[1]: Entering directory '/work'
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
[05/11/2024-07:36:40] [TRT] [I] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 43, GPU 480 (MiB)
[05/11/2024-07:36:48] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1958, GPU +346, now: CPU 2106, GPU 826 (MiB)
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 0) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 3) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 6) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 10) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 11) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 13) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 11) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 17) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 18) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 20) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 18) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 24) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 25) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 27) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 25) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 31) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 32) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 34) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 32) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 38) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 39) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 41) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 39) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 45) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 46) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 48) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 46) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 52) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 53) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 55) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 53) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 59) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 60) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 62) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 60) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 66) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 67) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 69) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 67) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:50] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 73) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 74) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 76) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 74) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 80) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 81) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 83) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 81) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 87) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 88) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 90) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 88) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 94) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 95) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 97) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 95) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 101) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 102) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 104) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 102) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 108) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 109) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 111) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 109) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 115) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 116) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 118) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 116) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 122) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 123) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 125) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 123) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 129) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 130) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 132) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 130) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 136) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 137) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 139) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 137) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 143) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 146) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 150) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 151) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 153) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 151) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 157) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 158) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 160) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 158) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:52] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-07:36:53] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 164) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:53] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:53] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 165) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:53] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:53] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 167) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:53] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 165) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-07:36:53] [TRT] [W] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.
[05/11/2024-07:36:53] [TRT] [W] Missing scale and zero-point for tensor input_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-07:36:53] [TRT] [W] Missing scale and zero-point for tensor segment_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-07:36:53] [TRT] [W] Missing scale and zero-point for tensor cu_seqlens, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-07:36:53] [TRT] [W] Missing scale and zero-point for tensor max_seqlen, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-07:36:53] [TRT] [I] Graph optimization time: 0.00501545 seconds.
[05/11/2024-07:36:53] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +7, GPU +8, now: CPU 3904, GPU 958 (MiB)
[05/11/2024-07:36:53] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 3905, GPU 968 (MiB)
[05/11/2024-07:36:53] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[05/11/2024-07:37:06] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.
[05/11/2024-07:37:06] [TRT] [I] Detected 4 inputs and 1 output network tensors.
[05/11/2024-07:37:12] [TRT] [I] Total Host Persistent Memory: 152160
[05/11/2024-07:37:12] [TRT] [I] Total Device Persistent Memory: 0
[05/11/2024-07:37:12] [TRT] [I] Total Scratch Memory: 0
[05/11/2024-07:37:12] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 174 steps to complete.
[05/11/2024-07:37:12] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 2.11967ms to assign 7 blocks to 174 nodes requiring 2754048 bytes.
[05/11/2024-07:37:12] [TRT] [I] Total Activation Memory: 2753024
[05/11/2024-07:37:16] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.
[05/11/2024-07:37:16] [TRT] [I] Detected 4 inputs and 1 output network tensors.
[05/11/2024-07:37:21] [TRT] [I] Total Host Persistent Memory: 152160
[05/11/2024-07:37:21] [TRT] [I] Total Device Persistent Memory: 0
[05/11/2024-07:37:21] [TRT] [I] Total Scratch Memory: 0
[05/11/2024-07:37:21] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 174 steps to complete.
[05/11/2024-07:37:21] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 2.0384ms to assign 7 blocks to 174 nodes requiring 2754048 bytes.
[05/11/2024-07:37:21] [TRT] [I] Total Activation Memory: 2754048
[05/11/2024-07:37:21] [TRT] [I] Total Weights Memory: 102121992
[05/11/2024-07:37:21] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 4638, GPU 1210 (MiB)
[05/11/2024-07:37:21] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 4638, GPU 1218 (MiB)
[05/11/2024-07:37:21] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 32 MiB, GPU 214 MiB
[05/11/2024-07:37:22] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +102, now: CPU 0, GPU 102 (MiB)
[05/11/2024-07:37:22] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 6041 MiB
{'benchmark': <Benchmark.BERT: AliasedName(name='bert', aliases=(), patterns=())>, 'bert_opt_seqlen': 384, 'buffer_manager_thread_count': 0, 'data_dir': '/scratch/gonisla//data', 'enable_interleaved': False, 'gpu_batch_size': 1, 'gpu_copy_streams': 1, 'gpu_inference_streams': 2, 'input_dtype': 'int32', 'input_format': 'linear', 'log_dir': '/work/build/logs/2024.05.11-07.36.26', 'precision': 'int8', 'preprocessed_data_dir': '/scratch/gonisla//preprocessed_data', 'scenario': <Scenario.Server: AliasedName(name='Server', aliases=(), patterns=())>, 'system': SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=16, threads_per_core=1): 1}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=65.551536, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=65551536000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA A30', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=24.0, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=25769803776), max_power_limit=165.0, pci_id='0x20B710DE', compute_sm=80): 1})), numa_conf=None, system_id='ocejon'), 'use_graphs': True, 'use_small_tile_gemm_plugin': True, 'system_id': 'ocejon', 'config_name': 'ocejon_bert_Server', 'workload_setting': WorkloadSetting(harness_type=<HarnessType.Custom: AliasedName(name='custom', aliases=(), patterns=())>, accuracy_target=<AccuracyTarget.k_99: 0.99>, power_setting=<PowerSetting.MaxP: AliasedName(name='MaxP', aliases=(), patterns=())>), 'optimization_level': 'plugin-enabled', 'use_cpu': False, 'use_inferentia': False, 'num_profiles': 1, 'config_ver': 'custom_k_99_MaxP', 'accuracy_level': '99%', 'inference_server': 'custom', 'power_limit': None, 'cpu_freq': None, 'batch_size': 1, 'dla_core': None}
This is the path: -----> 
build/models/bert/bert_large_v1_1_fake_quant.onnx
Replacing l0_fc_qkv with small-tile GEMM plugin.
Replacing l0_fc_aout with small-tile GEMM plugin.
Replacing l0_fc_mid_gelu with small-tile GEMM plugin.
Replacing l1_fc_qkv with small-tile GEMM plugin.
Replacing l1_fc_aout with small-tile GEMM plugin.
Replacing l1_fc_mid_gelu with small-tile GEMM plugin.
Replacing l2_fc_qkv with small-tile GEMM plugin.
Replacing l2_fc_aout with small-tile GEMM plugin.
Replacing l2_fc_mid_gelu with small-tile GEMM plugin.
Replacing l3_fc_qkv with small-tile GEMM plugin.
Replacing l3_fc_aout with small-tile GEMM plugin.
Replacing l3_fc_mid_gelu with small-tile GEMM plugin.
Replacing l4_fc_qkv with small-tile GEMM plugin.
Replacing l4_fc_aout with small-tile GEMM plugin.
Replacing l4_fc_mid_gelu with small-tile GEMM plugin.
Replacing l5_fc_qkv with small-tile GEMM plugin.
Replacing l5_fc_aout with small-tile GEMM plugin.
Replacing l5_fc_mid_gelu with small-tile GEMM plugin.
Replacing l6_fc_qkv with small-tile GEMM plugin.
Replacing l6_fc_aout with small-tile GEMM plugin.
Replacing l6_fc_mid_gelu with small-tile GEMM plugin.
Replacing l7_fc_qkv with small-tile GEMM plugin.
Replacing l7_fc_aout with small-tile GEMM plugin.
Replacing l7_fc_mid_gelu with small-tile GEMM plugin.
Replacing l8_fc_qkv with small-tile GEMM plugin.
Replacing l8_fc_aout with small-tile GEMM plugin.
Replacing l8_fc_mid_gelu with small-tile GEMM plugin.
Replacing l9_fc_qkv with small-tile GEMM plugin.
Replacing l9_fc_aout with small-tile GEMM plugin.
Replacing l9_fc_mid_gelu with small-tile GEMM plugin.
Replacing l10_fc_qkv with small-tile GEMM plugin.
Replacing l10_fc_aout with small-tile GEMM plugin.
Replacing l10_fc_mid_gelu with small-tile GEMM plugin.
Replacing l11_fc_qkv with small-tile GEMM plugin.
Replacing l11_fc_aout with small-tile GEMM plugin.
Replacing l11_fc_mid_gelu with small-tile GEMM plugin.
Replacing l12_fc_qkv with small-tile GEMM plugin.
Replacing l12_fc_aout with small-tile GEMM plugin.
Replacing l12_fc_mid_gelu with small-tile GEMM plugin.
Replacing l13_fc_qkv with small-tile GEMM plugin.
Replacing l13_fc_aout with small-tile GEMM plugin.
Replacing l13_fc_mid_gelu with small-tile GEMM plugin.
Replacing l14_fc_qkv with small-tile GEMM plugin.
Replacing l14_fc_aout with small-tile GEMM plugin.
Replacing l14_fc_mid_gelu with small-tile GEMM plugin.
Replacing l15_fc_qkv with small-tile GEMM plugin.
Replacing l15_fc_aout with small-tile GEMM plugin.
Replacing l15_fc_mid_gelu with small-tile GEMM plugin.
Replacing l16_fc_qkv with small-tile GEMM plugin.
Replacing l16_fc_aout with small-tile GEMM plugin.
Replacing l16_fc_mid_gelu with small-tile GEMM plugin.
Replacing l17_fc_qkv with small-tile GEMM plugin.
Replacing l17_fc_aout with small-tile GEMM plugin.
Replacing l17_fc_mid_gelu with small-tile GEMM plugin.
Replacing l18_fc_qkv with small-tile GEMM plugin.
Replacing l18_fc_aout with small-tile GEMM plugin.
Replacing l18_fc_mid_gelu with small-tile GEMM plugin.
Replacing l19_fc_qkv with small-tile GEMM plugin.
Replacing l19_fc_aout with small-tile GEMM plugin.
Replacing l19_fc_mid_gelu with small-tile GEMM plugin.
Replacing l20_fc_qkv with small-tile GEMM plugin.
Replacing l20_fc_aout with small-tile GEMM plugin.
Replacing l20_fc_mid_gelu with small-tile GEMM plugin.
Replacing l21_fc_qkv with small-tile GEMM plugin.
Replacing l21_fc_aout with small-tile GEMM plugin.
Replacing l21_fc_mid_gelu with small-tile GEMM plugin.
Replacing l22_fc_qkv with small-tile GEMM plugin.
Replacing l22_fc_aout with small-tile GEMM plugin.
Replacing l22_fc_mid_gelu with small-tile GEMM plugin.
Replacing l23_fc_qkv with small-tile GEMM plugin.
Replacing l23_fc_aout with small-tile GEMM plugin.
Replacing l23_fc_mid_gelu with small-tile GEMM plugin.
Time taken to generate engines: 47.84139680862427 seconds
make[1]: Leaving directory '/work'
make[1]: Entering directory '/work'
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
