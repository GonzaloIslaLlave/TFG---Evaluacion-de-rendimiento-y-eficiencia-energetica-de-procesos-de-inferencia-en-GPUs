make[1]: Entering directory '/work'
[03/20/2024-04:02:37] [TRT] [I] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 43, GPU 480 (MiB)
[03/20/2024-04:02:45] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1958, GPU +346, now: CPU 2106, GPU 826 (MiB)
[03/20/2024-04:02:46] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 0) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 3) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 6) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 10) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 11) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 13) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 11) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 17) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 18) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 20) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 18) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 24) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 25) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 27) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 25) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 31) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 32) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 34) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 32) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 38) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 39) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 41) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 39) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 45) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 46) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 48) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 46) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 52) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 53) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 55) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 53) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 59) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 60) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 62) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 60) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 66) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 67) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 69) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 67) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 73) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 74) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 76) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 74) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 80) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 81) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 83) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 81) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 87) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 88) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 90) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 88) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:48] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 94) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 95) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 97) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 95) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 101) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 102) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 104) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 102) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 108) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 109) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 111) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 109) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 115) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 116) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 118) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 116) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 122) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 123) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 125) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 123) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 129) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 130) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 132) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 130) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 136) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 137) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 139) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:49] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 137) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 143) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 146) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 150) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 151) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 153) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 151) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 157) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 158) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 160) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 158) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [I] Using default for use_int8_scale_max: true
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 164) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 165) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_gelu_out. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 167) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 165) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[03/20/2024-04:02:50] [TRT] [W] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.
[03/20/2024-04:02:50] [TRT] [W] Missing scale and zero-point for tensor input_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[03/20/2024-04:02:50] [TRT] [W] Missing scale and zero-point for tensor segment_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[03/20/2024-04:02:50] [TRT] [W] Missing scale and zero-point for tensor cu_seqlens, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[03/20/2024-04:02:50] [TRT] [W] Missing scale and zero-point for tensor max_seqlen, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[03/20/2024-04:02:50] [TRT] [I] Graph optimization time: 0.00498014 seconds.
[03/20/2024-04:02:50] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +7, GPU +8, now: CPU 3904, GPU 958 (MiB)
[03/20/2024-04:02:50] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 3905, GPU 968 (MiB)
[03/20/2024-04:02:50] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[03/20/2024-04:09:25] [TRT] [I] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[03/20/2024-04:11:58] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.
[03/20/2024-04:11:58] [TRT] [I] Detected 4 inputs and 1 output network tensors.
[03/20/2024-04:12:02] [TRT] [I] Total Host Persistent Memory: 165984
[03/20/2024-04:12:02] [TRT] [I] Total Device Persistent Memory: 0
[03/20/2024-04:12:02] [TRT] [I] Total Scratch Memory: 0
[03/20/2024-04:12:02] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 222 steps to complete.
[03/20/2024-04:12:02] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 3.35282ms to assign 7 blocks to 222 nodes requiring 1409287680 bytes.
[03/20/2024-04:12:02] [TRT] [I] Total Activation Memory: 1409286656
[03/20/2024-04:12:06] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.
[03/20/2024-04:12:06] [TRT] [I] Detected 4 inputs and 1 output network tensors.
[03/20/2024-04:12:09] [TRT] [I] Total Host Persistent Memory: 165984
[03/20/2024-04:12:09] [TRT] [I] Total Device Persistent Memory: 0
[03/20/2024-04:12:09] [TRT] [I] Total Scratch Memory: 0
[03/20/2024-04:12:09] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 222 steps to complete.
[03/20/2024-04:12:09] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 3.27538ms to assign 7 blocks to 222 nodes requiring 1409287680 bytes.
[03/20/2024-04:12:09] [TRT] [I] Total Activation Memory: 1409287680
[03/20/2024-04:12:09] [TRT] [I] Total Weights Memory: 102106632
[03/20/2024-04:12:09] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 4639, GPU 1210 (MiB)
[03/20/2024-04:12:09] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 4639, GPU 1218 (MiB)
[03/20/2024-04:12:09] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 8 MiB, GPU 8467 MiB
[03/20/2024-04:12:09] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +102, now: CPU 0, GPU 102 (MiB)
[03/20/2024-04:12:09] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 6049 MiB
{'benchmark': <Benchmark.BERT: AliasedName(name='bert', aliases=(), patterns=())>, 'bert_opt_seqlen': 384, 'buffer_manager_thread_count': 0, 'data_dir': '/scratch/gonisla//data', 'enable_interleaved': False, 'gpu_batch_size': 512, 'gpu_copy_streams': 1, 'gpu_inference_streams': 2, 'input_dtype': 'int32', 'input_format': 'linear', 'log_dir': '/work/build/logs/2024.03.20-04.02.34', 'precision': 'int8', 'preprocessed_data_dir': '/scratch/gonisla//preprocessed_data', 'scenario': <Scenario.Server: AliasedName(name='Server', aliases=(), patterns=())>, 'system': SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=16, threads_per_core=1): 1}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=65.551536, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=65551536000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA A30', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=24.0, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=25769803776), max_power_limit=165.0, pci_id='0x20B710DE', compute_sm=80): 1})), numa_conf=None, system_id='ocejon'), 'use_graphs': True, 'use_small_tile_gemm_plugin': True, 'system_id': 'ocejon', 'config_name': 'ocejon_bert_Server', 'workload_setting': WorkloadSetting(harness_type=<HarnessType.Custom: AliasedName(name='custom', aliases=(), patterns=())>, accuracy_target=<AccuracyTarget.k_99: 0.99>, power_setting=<PowerSetting.MaxP: AliasedName(name='MaxP', aliases=(), patterns=())>), 'optimization_level': 'plugin-enabled', 'use_cpu': False, 'use_inferentia': False, 'num_profiles': 1, 'config_ver': 'custom_k_99_MaxP', 'accuracy_level': '99%', 'inference_server': 'custom', 'power_limit': None, 'cpu_freq': None, 'batch_size': 512, 'dla_core': None}
This is the path: -----> 
build/models/bert/bert_large_v1_1_fake_quant.onnx
Replacing l0_fc_qkv with small-tile GEMM plugin.
Replacing l0_fc_aout with small-tile GEMM plugin.
Replacing l0_fc_mid_gelu with small-tile GEMM plugin.
Replacing l1_fc_qkv with small-tile GEMM plugin.
Replacing l1_fc_aout with small-tile GEMM plugin.
Replacing l1_fc_mid_gelu with small-tile GEMM plugin.
Replacing l2_fc_qkv with small-tile GEMM plugin.
Replacing l2_fc_aout with small-tile GEMM plugin.
Replacing l2_fc_mid_gelu with small-tile GEMM plugin.
Replacing l3_fc_qkv with small-tile GEMM plugin.
Replacing l3_fc_aout with small-tile GEMM plugin.
Replacing l3_fc_mid_gelu with small-tile GEMM plugin.
Replacing l4_fc_qkv with small-tile GEMM plugin.
Replacing l4_fc_aout with small-tile GEMM plugin.
Replacing l4_fc_mid_gelu with small-tile GEMM plugin.
Replacing l5_fc_qkv with small-tile GEMM plugin.
Replacing l5_fc_aout with small-tile GEMM plugin.
Replacing l5_fc_mid_gelu with small-tile GEMM plugin.
Replacing l6_fc_qkv with small-tile GEMM plugin.
Replacing l6_fc_aout with small-tile GEMM plugin.
Replacing l6_fc_mid_gelu with small-tile GEMM plugin.
Replacing l7_fc_qkv with small-tile GEMM plugin.
Replacing l7_fc_aout with small-tile GEMM plugin.
Replacing l7_fc_mid_gelu with small-tile GEMM plugin.
Replacing l8_fc_qkv with small-tile GEMM plugin.
Replacing l8_fc_aout with small-tile GEMM plugin.
Replacing l8_fc_mid_gelu with small-tile GEMM plugin.
Replacing l9_fc_qkv with small-tile GEMM plugin.
Replacing l9_fc_aout with small-tile GEMM plugin.
Replacing l9_fc_mid_gelu with small-tile GEMM plugin.
Replacing l10_fc_qkv with small-tile GEMM plugin.
Replacing l10_fc_aout with small-tile GEMM plugin.
Replacing l10_fc_mid_gelu with small-tile GEMM plugin.
Replacing l11_fc_qkv with small-tile GEMM plugin.
Replacing l11_fc_aout with small-tile GEMM plugin.
Replacing l11_fc_mid_gelu with small-tile GEMM plugin.
Replacing l12_fc_qkv with small-tile GEMM plugin.
Replacing l12_fc_aout with small-tile GEMM plugin.
Replacing l12_fc_mid_gelu with small-tile GEMM plugin.
Replacing l13_fc_qkv with small-tile GEMM plugin.
Replacing l13_fc_aout with small-tile GEMM plugin.
Replacing l13_fc_mid_gelu with small-tile GEMM plugin.
Replacing l14_fc_qkv with small-tile GEMM plugin.
Replacing l14_fc_aout with small-tile GEMM plugin.
Replacing l14_fc_mid_gelu with small-tile GEMM plugin.
Replacing l15_fc_qkv with small-tile GEMM plugin.
Replacing l15_fc_aout with small-tile GEMM plugin.
Replacing l15_fc_mid_gelu with small-tile GEMM plugin.
Replacing l16_fc_qkv with small-tile GEMM plugin.
Replacing l16_fc_aout with small-tile GEMM plugin.
Replacing l16_fc_mid_gelu with small-tile GEMM plugin.
Replacing l17_fc_qkv with small-tile GEMM plugin.
Replacing l17_fc_aout with small-tile GEMM plugin.
Replacing l17_fc_mid_gelu with small-tile GEMM plugin.
Replacing l18_fc_qkv with small-tile GEMM plugin.
Replacing l18_fc_aout with small-tile GEMM plugin.
Replacing l18_fc_mid_gelu with small-tile GEMM plugin.
Replacing l19_fc_qkv with small-tile GEMM plugin.
Replacing l19_fc_aout with small-tile GEMM plugin.
Replacing l19_fc_mid_gelu with small-tile GEMM plugin.
Replacing l20_fc_qkv with small-tile GEMM plugin.
Replacing l20_fc_aout with small-tile GEMM plugin.
Replacing l20_fc_mid_gelu with small-tile GEMM plugin.
Replacing l21_fc_qkv with small-tile GEMM plugin.
Replacing l21_fc_aout with small-tile GEMM plugin.
Replacing l21_fc_mid_gelu with small-tile GEMM plugin.
Replacing l22_fc_qkv with small-tile GEMM plugin.
Replacing l22_fc_aout with small-tile GEMM plugin.
Replacing l22_fc_mid_gelu with small-tile GEMM plugin.
Replacing l23_fc_qkv with small-tile GEMM plugin.
Replacing l23_fc_aout with small-tile GEMM plugin.
Replacing l23_fc_mid_gelu with small-tile GEMM plugin.
Time taken to generate engines: 578.0965230464935 seconds
make[1]: Leaving directory '/work'
make[1]: Entering directory '/work'
[2024-03-20 04:12:19,336 main.py:230 INFO] Detected system ID: KnownSystem.ocejon
[2024-03-20 04:12:19,985 generate_conf_files.py:107 INFO] Generated measurements/ entries for ocejon_TRT/bert-99/Server
[2024-03-20 04:12:19,986 __init__.py:46 INFO] Running command: ./build/bin/harness_bert --logfile_outdir="/work/build/logs/2024.03.20-04.02.34/ocejon_TRT/bert-99/Server" --logfile_prefix="mlperf_log_" --performance_sample_count=10833 --gpu_batch_size=512 --tensor_path="build/preprocessed_data/squad_tokenized/input_ids.npy,build/preprocessed_data/squad_tokenized/segment_ids.npy,build/preprocessed_data/squad_tokenized/input_mask.npy" --use_graphs=true --gpu_inference_streams=2 --gpu_copy_streams=1 --gpu_engines="./build/engines/ocejon/bert/Server/bert-Server-gpu-int8_S_384_B_512_P_2_vs.custom_k_99_MaxP.plan" --mlperf_conf_path="build/loadgen-configs/ocejon_TRT/bert-99/Server/mlperf.conf" --user_conf_path="build/loadgen-configs/ocejon_TRT/bert-99/Server/user.conf" --scenario Server --model bert
[2024-03-20 04:12:19,986 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.BERT
buffer_manager_thread_count : 0
coalesced_tensor : True
data_dir : /scratch/gonisla//data
enable_interleaved : False
gpu_batch_size : 512
gpu_copy_streams : 1
gpu_inference_streams : 2
input_dtype : int32
input_format : linear
log_dir : /work/build/logs/2024.03.20-04.02.34
precision : int8
preprocessed_data_dir : /scratch/gonisla//preprocessed_data
scenario : Scenario.Server
server_target_qps : 1000
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=16, threads_per_core=1): 1}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=65.551536, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=65551536000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA A30', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=24.0, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=25769803776), max_power_limit=165.0, pci_id='0x20B710DE', compute_sm=80): 1})), numa_conf=None, system_id='ocejon')
tensor_path : build/preprocessed_data/squad_tokenized/input_ids.npy,build/preprocessed_data/squad_tokenized/segment_ids.npy,build/preprocessed_data/squad_tokenized/input_mask.npy
use_graphs : True
use_small_tile_gemm_plugin : True
system_id : ocejon
config_name : ocejon_bert_Server
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
use_cpu : False
use_inferentia : False
num_profiles : 1
config_ver : custom_k_99_MaxP
accuracy_level : 99%
inference_server : custom
skip_file_checks : True
power_limit : None
cpu_freq : None
&&&& RUNNING BERT_HARNESS # ./build/bin/harness_bert
I0320 04:12:20.038550 10976 main_bert.cc:163] Found 1 GPUs
I0320 04:12:20.971633 10976 bert_server.cc:142] Engine Path: ./build/engines/ocejon/bert/Server/bert-Server-gpu-int8_S_384_B_512_P_2_vs.custom_k_99_MaxP.plan
[I] [TRT] Loaded engine size: 608 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +7, GPU +8, now: CPU 1168, GPU 712 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 1169, GPU 722 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +97, now: CPU 0, GPU 97 (MiB)
I0320 04:12:21.890379 10976 bert_server.cc:203] Engines Creation Completed
I0320 04:12:21.891398 10976 bert_server.cc:208] Use CUDA graphs
I0320 04:12:21.892575 10976 bert_core_vs.cc:385] Engine - Device Memory requirements: 1409287680
I0320 04:12:21.892582 10976 bert_core_vs.cc:393] Engine - Number of Optimization Profiles: 2
I0320 04:12:21.892588 10976 bert_core_vs.cc:415] Engine - Profile 0 maxDims 196608 Bmax=512 Smax=384
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 560, GPU 2060 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 561, GPU 2068 (MiB)
I0320 04:12:22.445508 10976 bert_core_vs.cc:426] Setting Opt.Prof. to 0
I0320 04:12:22.445530 10976 bert_core_vs.cc:444] Context creation complete. Max supported batchSize: 512
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 97 (MiB)
I0320 04:12:22.446413 10976 bert_core_vs.cc:476] Setup complete
F0320 08:53:14.841223 10981 bert_core_vs.cc:332] Check failed: cudaStreamSynchronize(mStream) == CUDA_SUCCESS (719 vs. 0) 
*** Check failure stack trace: ***
    @     0x7f9760f0af00  google::LogMessage::Fail()
    @     0x7f9760f0ae3b  google::LogMessage::SendToLog()
    @     0x7f9760f0a76c  google::LogMessage::Flush()
    @     0x7f9760f0dd7a  google::LogMessageFatal::~LogMessageFatal()
    @     0x559ea4c5629b  BERTCoreVS::BuildGraphs()
    @     0x7f9760db0df4  (unknown)
    @     0x7f9760ec4609  start_thread
    @     0x7f9760a9d353  clone
    @              (nil)  (unknown)
Aborted (core dumped)
Traceback (most recent call last):
  File "/usr/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/work/code/main.py", line 232, in <module>
    main(main_args, DETECTED_SYSTEM)
  File "/work/code/main.py", line 145, in main
    dispatch_action(main_args, config_dict, workload_setting)
  File "/work/code/main.py", line 203, in dispatch_action
    handler.run()
  File "/work/code/actionhandler/base.py", line 82, in run
    self.handle_failure()
  File "/work/code/actionhandler/run_harness.py", line 193, in handle_failure
    raise RuntimeError("Run harness failed!")
RuntimeError: Run harness failed!
Traceback (most recent call last):
  File "/work/code/actionhandler/run_harness.py", line 162, in handle
    result_data = self.harness.run_harness(flag_dict=self.harness_flag_dict, skip_generate_measurements=True)
  File "/work/code/common/harness.py", line 339, in run_harness
    output = run_command(self._construct_terminal_command(argstr), get_output=True, custom_env=self.env_vars)
  File "/work/code/common/__init__.py", line 67, in run_command
    raise subprocess.CalledProcessError(ret, cmd)
subprocess.CalledProcessError: Command './build/bin/harness_bert --logfile_outdir="/work/build/logs/2024.03.20-04.02.34/ocejon_TRT/bert-99/Server" --logfile_prefix="mlperf_log_" --performance_sample_count=10833 --gpu_batch_size=512 --tensor_path="build/preprocessed_data/squad_tokenized/input_ids.npy,build/preprocessed_data/squad_tokenized/segment_ids.npy,build/preprocessed_data/squad_tokenized/input_mask.npy" --use_graphs=true --gpu_inference_streams=2 --gpu_copy_streams=1 --gpu_engines="./build/engines/ocejon/bert/Server/bert-Server-gpu-int8_S_384_B_512_P_2_vs.custom_k_99_MaxP.plan" --mlperf_conf_path="build/loadgen-configs/ocejon_TRT/bert-99/Server/mlperf.conf" --user_conf_path="build/loadgen-configs/ocejon_TRT/bert-99/Server/user.conf" --scenario Server --model bert' returned non-zero exit status 134.
make[1]: Leaving directory '/work'
