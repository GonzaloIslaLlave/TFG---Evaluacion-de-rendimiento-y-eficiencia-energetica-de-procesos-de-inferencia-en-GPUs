$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
make[1]: Entering directory '/work'
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
$ARCH is [x86_64]
$SYSTEM_NAME is [KnownSystem.ocejon]
$IS_SOC is [0]
$USE_CPU is [0]
$USE_INFERENTIA is [0]
[05/11/2024-21:10:07] [TRT] [I] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 43, GPU 480 (MiB)
[05/11/2024-21:10:15] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1958, GPU +346, now: CPU 2106, GPU 826 (MiB)
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 0) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 3) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs l0_fc_mid_out and (Unnamed Layer* 6) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 11) [ElementWise]_output and (Unnamed Layer* 7) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 13) [ElementWise]_output and (Unnamed Layer* 8) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 15) [Activation]_output and (Unnamed Layer* 9) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 16) [ElementWise]_output and (Unnamed Layer* 10) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 19) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 23) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 24) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs l1_fc_mid_out and (Unnamed Layer* 26) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 31) [ElementWise]_output and (Unnamed Layer* 27) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 33) [ElementWise]_output and (Unnamed Layer* 28) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 35) [Activation]_output and (Unnamed Layer* 29) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 36) [ElementWise]_output and (Unnamed Layer* 30) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 39) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 24) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 43) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 44) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs l2_fc_mid_out and (Unnamed Layer* 46) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 51) [ElementWise]_output and (Unnamed Layer* 47) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 53) [ElementWise]_output and (Unnamed Layer* 48) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 55) [Activation]_output and (Unnamed Layer* 49) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 56) [ElementWise]_output and (Unnamed Layer* 50) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 59) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 44) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 63) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 64) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs l3_fc_mid_out and (Unnamed Layer* 66) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 71) [ElementWise]_output and (Unnamed Layer* 67) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 73) [ElementWise]_output and (Unnamed Layer* 68) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 75) [Activation]_output and (Unnamed Layer* 69) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 76) [ElementWise]_output and (Unnamed Layer* 70) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 79) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 64) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 83) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 84) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs l4_fc_mid_out and (Unnamed Layer* 86) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 91) [ElementWise]_output and (Unnamed Layer* 87) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 93) [ElementWise]_output and (Unnamed Layer* 88) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 95) [Activation]_output and (Unnamed Layer* 89) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 96) [ElementWise]_output and (Unnamed Layer* 90) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 99) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 84) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 103) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 104) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs l5_fc_mid_out and (Unnamed Layer* 106) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 111) [ElementWise]_output and (Unnamed Layer* 107) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 113) [ElementWise]_output and (Unnamed Layer* 108) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 115) [Activation]_output and (Unnamed Layer* 109) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 116) [ElementWise]_output and (Unnamed Layer* 110) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 119) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 104) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 123) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 124) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs l6_fc_mid_out and (Unnamed Layer* 126) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 131) [ElementWise]_output and (Unnamed Layer* 127) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 133) [ElementWise]_output and (Unnamed Layer* 128) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 135) [Activation]_output and (Unnamed Layer* 129) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 136) [ElementWise]_output and (Unnamed Layer* 130) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 139) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 124) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 143) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs l7_fc_mid_out and (Unnamed Layer* 146) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 151) [ElementWise]_output and (Unnamed Layer* 147) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 153) [ElementWise]_output and (Unnamed Layer* 148) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 155) [Activation]_output and (Unnamed Layer* 149) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 156) [ElementWise]_output and (Unnamed Layer* 150) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 159) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:16] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 163) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 164) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l8_fc_mid_out and (Unnamed Layer* 166) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 171) [ElementWise]_output and (Unnamed Layer* 167) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 173) [ElementWise]_output and (Unnamed Layer* 168) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 175) [Activation]_output and (Unnamed Layer* 169) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 176) [ElementWise]_output and (Unnamed Layer* 170) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 179) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 164) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 183) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 184) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l9_fc_mid_out and (Unnamed Layer* 186) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 191) [ElementWise]_output and (Unnamed Layer* 187) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 193) [ElementWise]_output and (Unnamed Layer* 188) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 195) [Activation]_output and (Unnamed Layer* 189) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 196) [ElementWise]_output and (Unnamed Layer* 190) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 199) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 184) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 203) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 204) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l10_fc_mid_out and (Unnamed Layer* 206) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 211) [ElementWise]_output and (Unnamed Layer* 207) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 213) [ElementWise]_output and (Unnamed Layer* 208) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 215) [Activation]_output and (Unnamed Layer* 209) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 216) [ElementWise]_output and (Unnamed Layer* 210) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 219) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 204) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 223) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 224) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l11_fc_mid_out and (Unnamed Layer* 226) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 231) [ElementWise]_output and (Unnamed Layer* 227) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 233) [ElementWise]_output and (Unnamed Layer* 228) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 235) [Activation]_output and (Unnamed Layer* 229) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 236) [ElementWise]_output and (Unnamed Layer* 230) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 239) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 224) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 243) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 244) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l12_fc_mid_out and (Unnamed Layer* 246) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 251) [ElementWise]_output and (Unnamed Layer* 247) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 253) [ElementWise]_output and (Unnamed Layer* 248) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 255) [Activation]_output and (Unnamed Layer* 249) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 256) [ElementWise]_output and (Unnamed Layer* 250) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 259) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 244) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 263) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 264) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l13_fc_mid_out and (Unnamed Layer* 266) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 271) [ElementWise]_output and (Unnamed Layer* 267) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 273) [ElementWise]_output and (Unnamed Layer* 268) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 275) [Activation]_output and (Unnamed Layer* 269) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 276) [ElementWise]_output and (Unnamed Layer* 270) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 279) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 264) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 283) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 284) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l14_fc_mid_out and (Unnamed Layer* 286) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 291) [ElementWise]_output and (Unnamed Layer* 287) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 293) [ElementWise]_output and (Unnamed Layer* 288) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 295) [Activation]_output and (Unnamed Layer* 289) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 296) [ElementWise]_output and (Unnamed Layer* 290) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 299) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 284) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 303) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 304) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l15_fc_mid_out and (Unnamed Layer* 306) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 311) [ElementWise]_output and (Unnamed Layer* 307) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 313) [ElementWise]_output and (Unnamed Layer* 308) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 315) [Activation]_output and (Unnamed Layer* 309) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 316) [ElementWise]_output and (Unnamed Layer* 310) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 319) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 304) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 323) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 324) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l16_fc_mid_out and (Unnamed Layer* 326) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 331) [ElementWise]_output and (Unnamed Layer* 327) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 333) [ElementWise]_output and (Unnamed Layer* 328) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 335) [Activation]_output and (Unnamed Layer* 329) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 336) [ElementWise]_output and (Unnamed Layer* 330) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 339) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 324) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 343) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 344) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l17_fc_mid_out and (Unnamed Layer* 346) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 351) [ElementWise]_output and (Unnamed Layer* 347) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 353) [ElementWise]_output and (Unnamed Layer* 348) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 355) [Activation]_output and (Unnamed Layer* 349) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 356) [ElementWise]_output and (Unnamed Layer* 350) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 359) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 344) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 363) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 364) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l18_fc_mid_out and (Unnamed Layer* 366) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 371) [ElementWise]_output and (Unnamed Layer* 367) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 373) [ElementWise]_output and (Unnamed Layer* 368) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 375) [Activation]_output and (Unnamed Layer* 369) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 376) [ElementWise]_output and (Unnamed Layer* 370) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 379) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 364) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 383) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 384) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l19_fc_mid_out and (Unnamed Layer* 386) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 391) [ElementWise]_output and (Unnamed Layer* 387) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 393) [ElementWise]_output and (Unnamed Layer* 388) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 395) [Activation]_output and (Unnamed Layer* 389) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 396) [ElementWise]_output and (Unnamed Layer* 390) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 399) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 384) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 403) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 404) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l20_fc_mid_out and (Unnamed Layer* 406) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 411) [ElementWise]_output and (Unnamed Layer* 407) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 413) [ElementWise]_output and (Unnamed Layer* 408) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 415) [Activation]_output and (Unnamed Layer* 409) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 416) [ElementWise]_output and (Unnamed Layer* 410) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 419) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 404) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 423) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 424) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l21_fc_mid_out and (Unnamed Layer* 426) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 431) [ElementWise]_output and (Unnamed Layer* 427) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 433) [ElementWise]_output and (Unnamed Layer* 428) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 435) [Activation]_output and (Unnamed Layer* 429) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 436) [ElementWise]_output and (Unnamed Layer* 430) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 439) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 424) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 443) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 444) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l22_fc_mid_out and (Unnamed Layer* 446) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 451) [ElementWise]_output and (Unnamed Layer* 447) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 453) [ElementWise]_output and (Unnamed Layer* 448) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 455) [Activation]_output and (Unnamed Layer* 449) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 456) [ElementWise]_output and (Unnamed Layer* 450) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 459) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 444) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 463) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 464) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs l23_fc_mid_out and (Unnamed Layer* 466) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 471) [ElementWise]_output and (Unnamed Layer* 467) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 473) [ElementWise]_output and (Unnamed Layer* 468) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 475) [Activation]_output and (Unnamed Layer* 469) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 476) [ElementWise]_output and (Unnamed Layer* 470) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 479) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 464) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:10:17] [TRT] [W] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor input_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor segment_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor cu_seqlens, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor max_seqlen, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l0_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 6) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 7) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 8) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 9) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 10) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 11) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 12) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 13) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 14) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 15) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 16) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 17) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l1_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 26) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 27) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 28) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 29) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 30) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 31) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 32) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 33) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 34) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 35) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 36) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 37) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l2_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 46) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 47) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 48) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 49) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 50) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 51) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 52) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 53) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 54) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 55) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 56) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 57) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l3_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 66) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 67) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 68) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 69) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 70) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 71) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 72) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 73) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 74) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 75) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 76) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 77) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l4_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 86) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 87) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 88) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 89) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 90) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 91) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 92) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 93) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 94) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 95) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 96) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 97) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l5_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 106) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 107) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 108) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 109) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 110) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 111) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 112) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 113) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 114) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 115) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 116) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 117) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l6_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 126) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 127) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 128) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 129) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 130) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 131) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 132) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 133) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 134) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 135) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 136) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 137) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l7_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 146) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 147) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 148) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 149) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 150) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 151) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 152) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 153) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 154) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 155) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 156) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 157) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l8_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 166) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 167) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 168) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 169) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 170) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 171) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 172) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 173) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 174) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 175) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 176) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 177) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l9_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 186) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 187) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 188) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 189) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 190) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 191) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 192) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 193) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 194) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 195) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 196) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 197) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l10_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 206) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 207) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 208) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 209) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 210) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 211) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 212) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 213) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 214) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 215) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 216) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 217) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l11_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 226) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 227) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 228) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 229) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 230) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 231) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 232) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 233) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 234) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 235) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 236) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 237) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l12_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 246) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 247) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 248) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 249) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 250) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 251) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 252) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 253) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 254) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 255) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 256) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 257) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l13_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 266) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 267) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 268) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 269) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 270) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 271) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 272) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 273) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 274) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 275) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 276) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 277) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l14_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 286) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 287) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 288) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 289) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 290) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 291) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 292) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 293) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 294) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 295) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 296) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 297) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l15_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 306) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 307) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 308) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 309) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 310) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 311) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 312) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 313) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 314) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 315) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 316) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 317) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l16_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 326) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 327) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 328) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 329) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 330) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 331) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 332) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 333) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 334) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 335) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 336) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 337) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l17_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 346) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 347) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 348) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 349) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 350) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 351) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 352) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 353) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 354) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 355) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 356) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 357) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l18_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 366) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 367) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 368) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 369) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 370) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 371) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 372) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 373) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 374) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 375) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 376) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 377) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l19_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 386) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 387) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 388) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 389) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 390) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 391) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 392) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 393) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 394) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 395) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 396) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 397) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l20_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 406) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 407) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 408) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 409) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 410) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 411) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 412) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 413) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 414) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 415) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 416) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 417) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l21_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 426) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 427) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 428) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 429) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 430) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 431) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 432) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 433) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 434) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 435) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 436) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 437) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l22_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 446) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 447) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 448) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 449) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 450) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 451) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 452) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 453) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 454) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 455) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 456) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 457) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor l23_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 466) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 467) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 468) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 469) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 470) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 471) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 472) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 473) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 474) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 475) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 476) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 477) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:10:18] [TRT] [I] Graph optimization time: 1.58945 seconds.
[05/11/2024-21:10:18] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +8, now: CPU 3814, GPU 958 (MiB)
[05/11/2024-21:10:18] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 3815, GPU 968 (MiB)
[05/11/2024-21:10:18] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 30000 detected for tactic 0xf067e6205da31c2e.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 1 due to insufficient memory on requested size of 30000 detected for tactic 0x503619c69ae500ff.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 2 due to insufficient memory on requested size of 30000 detected for tactic 0xa8ef60e712f8ad24.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 3 due to insufficient memory on requested size of 30000 detected for tactic 0x3f243c490d502deb.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 4 due to insufficient memory on requested size of 30000 detected for tactic 0x9808072e706def96.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 5 due to insufficient memory on requested size of 30000 detected for tactic 0xa8609adc4e0ceb90.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 6 due to insufficient memory on requested size of 30000 detected for tactic 0xf64396b97c889179.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 7 due to insufficient memory on requested size of 30000 detected for tactic 0xc3cf6e1d1c6aff27.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020775.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 1 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002020e.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 2 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020419.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 3 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020777.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 4 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002076f.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 5 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020453.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 6 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020490.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 7 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020729.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 8 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000205f3.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 9 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020774.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 10 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002076c.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 11 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000200b3.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 12 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020592.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 13 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020442.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 14 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000200e8.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 15 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002038b.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 16 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002076b.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 17 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000203cc.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 18 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020732.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 19 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000203f3.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 20 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020771.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 21 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020623.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 22 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002054f.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 23 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020615.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 24 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002054a.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 25 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000204e4.
[05/11/2024-21:10:19] [TRT] [I] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 30000 detected for tactic 0x17173deba0b64484.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 30000 detected for tactic 0x3e2b881168d9689d.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 1 due to insufficient memory on requested size of 30000 detected for tactic 0xe47307053a42b3e4.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 2 due to insufficient memory on requested size of 30000 detected for tactic 0xf90060ce8193b811.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 3 due to insufficient memory on requested size of 30000 detected for tactic 0x7bc32c782b800c48.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 4 due to insufficient memory on requested size of 30000 detected for tactic 0xc7feb33970feefa7.
[05/11/2024-21:10:19] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:10:19] [TRT] [I] Skipping tactic 5 due to insufficient memory on requested size of 30000 detected for tactic 0xae0c89d047932ba3.
[05/11/2024-21:12:30] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:30] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 24375 detected for tactic 0x69c4e2ca38eadce2.
[05/11/2024-21:12:30] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:30] [TRT] [I] Skipping tactic 1 due to insufficient memory on requested size of 24375 detected for tactic 0xff6944b17d5b2e32.
[05/11/2024-21:12:30] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:30] [TRT] [I] Skipping tactic 2 due to insufficient memory on requested size of 24375 detected for tactic 0xa4ae2d82115c3e83.
[05/11/2024-21:12:30] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:30] [TRT] [I] Skipping tactic 3 due to insufficient memory on requested size of 24375 detected for tactic 0xfe80445f7bb61a99.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 24375 detected for tactic 0x2468d082d0ff7c9a.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 1 due to insufficient memory on requested size of 24375 detected for tactic 0x9003f5f7ff9b1aec.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 2 due to insufficient memory on requested size of 24375 detected for tactic 0x271b998fe31732ef.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 3 due to insufficient memory on requested size of 24375 detected for tactic 0xcd229658c16b33cd.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 4 due to insufficient memory on requested size of 24375 detected for tactic 0xa2f15b0a75b7dcec.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 5 due to insufficient memory on requested size of 24375 detected for tactic 0x91930a570b557437.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 6 due to insufficient memory on requested size of 24375 detected for tactic 0x445983715412fbda.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 7 due to insufficient memory on requested size of 24375 detected for tactic 0x991db9fd58152c33.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 8 due to insufficient memory on requested size of 24375 detected for tactic 0x84942841d92b0552.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 9 due to insufficient memory on requested size of 24375 detected for tactic 0x844ea9c00f711f19.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 10 due to insufficient memory on requested size of 24375 detected for tactic 0x1e55f8b415964e81.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 11 due to insufficient memory on requested size of 24375 detected for tactic 0xdc1f355deb032b87.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:50] [TRT] [I] Skipping tactic 12 due to insufficient memory on requested size of 24375 detected for tactic 0x5f1a472d416ff35e.
[05/11/2024-21:12:50] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 13 due to insufficient memory on requested size of 24375 detected for tactic 0xb5f710b331ba8377.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 14 due to insufficient memory on requested size of 24375 detected for tactic 0x748f926574b7bdc4.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 15 due to insufficient memory on requested size of 24375 detected for tactic 0xb86b920539eb9c79.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 16 due to insufficient memory on requested size of 24375 detected for tactic 0xdf7e1bd6a496d667.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 17 due to insufficient memory on requested size of 24375 detected for tactic 0xbeb5d91e1874a437.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 18 due to insufficient memory on requested size of 24375 detected for tactic 0x5bd8221bd57baf93.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 19 due to insufficient memory on requested size of 24375 detected for tactic 0x8141573686849b61.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 20 due to insufficient memory on requested size of 24375 detected for tactic 0x6d377e4222886190.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 21 due to insufficient memory on requested size of 24375 detected for tactic 0x9ec201b34455146e.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 22 due to insufficient memory on requested size of 24375 detected for tactic 0x960e9baa2a6cad5b.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 23 due to insufficient memory on requested size of 24375 detected for tactic 0x7720f198395e7d3d.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 24 due to insufficient memory on requested size of 24375 detected for tactic 0x33a5c6dd086942c1.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 25 due to insufficient memory on requested size of 24375 detected for tactic 0x65fbe45b4cb1d8a5.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 26 due to insufficient memory on requested size of 24375 detected for tactic 0x60b880e28fee7a0c.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 27 due to insufficient memory on requested size of 24375 detected for tactic 0x6986b0c6a136276e.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 28 due to insufficient memory on requested size of 24375 detected for tactic 0xcf64a2ae51bf6b36.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 29 due to insufficient memory on requested size of 24375 detected for tactic 0xe742f4598442d2f1.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 30 due to insufficient memory on requested size of 24375 detected for tactic 0xa71946688cad8664.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 31 due to insufficient memory on requested size of 24375 detected for tactic 0x3f2b14dec582741e.
[05/11/2024-21:12:51] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 32 due to insufficient memory on requested size of 24375 detected for tactic 0x6e9c17a33c93d9b0.
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 0xcba01f45d37a76a0 due to exception an illegal memory access was encountered
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 0x264d83735b1cba13 due to exception an illegal memory access was encountered
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 0x8e1dd2962c589dd4 due to exception an illegal memory access was encountered
[05/11/2024-21:12:51] [TRT] [I] Skipping tactic 0x7de8ad674a85e82a due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0xaba07efc49c11a6c due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0xd1a1d0d7a29590b7 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0xaceaca2c82ab40f3 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x27fb8661fea664b3 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x43e3804f158e597d due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x682fae8be946411d due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x4e7f02f91ddf4673 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0xaa30815cbd0f74b7 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x8f07802f58d278c0 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x758f8b2079a95b2e due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x68f52f0a3f1c5b56 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x6c336d1b7c64ffa3 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x86d1488f426124e1 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x5812ced7874b44d1 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0xb7a90e7097d64877 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x8486adb55ae0ca6c due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x62a338704beec449 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0xb34706d47a62a016 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x3e116db8858d0d41 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0xcf392eca056b8d88 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x70ffb5fe2e1321e3 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0xdb3ae212a7704d44 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0xd7985d83133bcf37 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x7524377e24bc511f due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x4f8662a723b489e1 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0x1cfa820c55616892 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [I] Skipping tactic 0xccdb99df0646c7b3 due to exception an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_qkv : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:12:52] [TRT] [E] /_src/plugin/common/bertCommon.h (300) - Cuda Error in operator(): 700 (an illegal memory access was encountered)
[05/11/2024-21:13:38] [TRT] [I] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 43, GPU 480 (MiB)
[05/11/2024-21:13:45] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1958, GPU +346, now: CPU 2106, GPU 826 (MiB)
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 0) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 3) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l0_fc_mid_out and (Unnamed Layer* 6) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 11) [ElementWise]_output and (Unnamed Layer* 7) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 13) [ElementWise]_output and (Unnamed Layer* 8) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 15) [Activation]_output and (Unnamed Layer* 9) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 16) [ElementWise]_output and (Unnamed Layer* 10) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 19) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 23) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 24) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l1_fc_mid_out and (Unnamed Layer* 26) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 31) [ElementWise]_output and (Unnamed Layer* 27) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 33) [ElementWise]_output and (Unnamed Layer* 28) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 35) [Activation]_output and (Unnamed Layer* 29) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 36) [ElementWise]_output and (Unnamed Layer* 30) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 39) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 24) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 43) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 44) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l2_fc_mid_out and (Unnamed Layer* 46) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 51) [ElementWise]_output and (Unnamed Layer* 47) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 53) [ElementWise]_output and (Unnamed Layer* 48) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 55) [Activation]_output and (Unnamed Layer* 49) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 56) [ElementWise]_output and (Unnamed Layer* 50) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 59) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 44) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 63) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 64) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l3_fc_mid_out and (Unnamed Layer* 66) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 71) [ElementWise]_output and (Unnamed Layer* 67) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 73) [ElementWise]_output and (Unnamed Layer* 68) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 75) [Activation]_output and (Unnamed Layer* 69) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 76) [ElementWise]_output and (Unnamed Layer* 70) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 79) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 64) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 83) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 84) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l4_fc_mid_out and (Unnamed Layer* 86) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 91) [ElementWise]_output and (Unnamed Layer* 87) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 93) [ElementWise]_output and (Unnamed Layer* 88) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 95) [Activation]_output and (Unnamed Layer* 89) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 96) [ElementWise]_output and (Unnamed Layer* 90) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 99) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 84) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 103) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 104) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l5_fc_mid_out and (Unnamed Layer* 106) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 111) [ElementWise]_output and (Unnamed Layer* 107) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 113) [ElementWise]_output and (Unnamed Layer* 108) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 115) [Activation]_output and (Unnamed Layer* 109) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 116) [ElementWise]_output and (Unnamed Layer* 110) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 119) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 104) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 123) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 124) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l6_fc_mid_out and (Unnamed Layer* 126) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 131) [ElementWise]_output and (Unnamed Layer* 127) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 133) [ElementWise]_output and (Unnamed Layer* 128) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 135) [Activation]_output and (Unnamed Layer* 129) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 136) [ElementWise]_output and (Unnamed Layer* 130) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 139) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 124) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 143) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l7_fc_mid_out and (Unnamed Layer* 146) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 151) [ElementWise]_output and (Unnamed Layer* 147) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 153) [ElementWise]_output and (Unnamed Layer* 148) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 155) [Activation]_output and (Unnamed Layer* 149) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 156) [ElementWise]_output and (Unnamed Layer* 150) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 159) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 163) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 164) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l8_fc_mid_out and (Unnamed Layer* 166) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 171) [ElementWise]_output and (Unnamed Layer* 167) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 173) [ElementWise]_output and (Unnamed Layer* 168) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 175) [Activation]_output and (Unnamed Layer* 169) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 176) [ElementWise]_output and (Unnamed Layer* 170) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 179) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 164) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 183) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 184) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l9_fc_mid_out and (Unnamed Layer* 186) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 191) [ElementWise]_output and (Unnamed Layer* 187) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 193) [ElementWise]_output and (Unnamed Layer* 188) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 195) [Activation]_output and (Unnamed Layer* 189) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 196) [ElementWise]_output and (Unnamed Layer* 190) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 199) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 184) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 203) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 204) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l10_fc_mid_out and (Unnamed Layer* 206) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 211) [ElementWise]_output and (Unnamed Layer* 207) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 213) [ElementWise]_output and (Unnamed Layer* 208) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 215) [Activation]_output and (Unnamed Layer* 209) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 216) [ElementWise]_output and (Unnamed Layer* 210) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 219) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 204) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 223) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 224) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l11_fc_mid_out and (Unnamed Layer* 226) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 231) [ElementWise]_output and (Unnamed Layer* 227) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 233) [ElementWise]_output and (Unnamed Layer* 228) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 235) [Activation]_output and (Unnamed Layer* 229) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 236) [ElementWise]_output and (Unnamed Layer* 230) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 239) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 224) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 243) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 244) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l12_fc_mid_out and (Unnamed Layer* 246) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 251) [ElementWise]_output and (Unnamed Layer* 247) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 253) [ElementWise]_output and (Unnamed Layer* 248) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 255) [Activation]_output and (Unnamed Layer* 249) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 256) [ElementWise]_output and (Unnamed Layer* 250) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 259) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 244) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 263) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 264) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l13_fc_mid_out and (Unnamed Layer* 266) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 271) [ElementWise]_output and (Unnamed Layer* 267) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 273) [ElementWise]_output and (Unnamed Layer* 268) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 275) [Activation]_output and (Unnamed Layer* 269) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 276) [ElementWise]_output and (Unnamed Layer* 270) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 279) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 264) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 283) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 284) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l14_fc_mid_out and (Unnamed Layer* 286) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 291) [ElementWise]_output and (Unnamed Layer* 287) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 293) [ElementWise]_output and (Unnamed Layer* 288) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 295) [Activation]_output and (Unnamed Layer* 289) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 296) [ElementWise]_output and (Unnamed Layer* 290) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 299) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 284) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 303) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 304) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l15_fc_mid_out and (Unnamed Layer* 306) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 311) [ElementWise]_output and (Unnamed Layer* 307) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 313) [ElementWise]_output and (Unnamed Layer* 308) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 315) [Activation]_output and (Unnamed Layer* 309) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 316) [ElementWise]_output and (Unnamed Layer* 310) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 319) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 304) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 323) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 324) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l16_fc_mid_out and (Unnamed Layer* 326) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 331) [ElementWise]_output and (Unnamed Layer* 327) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 333) [ElementWise]_output and (Unnamed Layer* 328) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 335) [Activation]_output and (Unnamed Layer* 329) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 336) [ElementWise]_output and (Unnamed Layer* 330) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 339) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 324) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 343) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 344) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l17_fc_mid_out and (Unnamed Layer* 346) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 351) [ElementWise]_output and (Unnamed Layer* 347) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 353) [ElementWise]_output and (Unnamed Layer* 348) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 355) [Activation]_output and (Unnamed Layer* 349) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 356) [ElementWise]_output and (Unnamed Layer* 350) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 359) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 344) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 363) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 364) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l18_fc_mid_out and (Unnamed Layer* 366) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 371) [ElementWise]_output and (Unnamed Layer* 367) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 373) [ElementWise]_output and (Unnamed Layer* 368) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 375) [Activation]_output and (Unnamed Layer* 369) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 376) [ElementWise]_output and (Unnamed Layer* 370) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 379) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 364) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 383) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 384) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l19_fc_mid_out and (Unnamed Layer* 386) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 391) [ElementWise]_output and (Unnamed Layer* 387) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 393) [ElementWise]_output and (Unnamed Layer* 388) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 395) [Activation]_output and (Unnamed Layer* 389) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 396) [ElementWise]_output and (Unnamed Layer* 390) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 399) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 384) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 403) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 404) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l20_fc_mid_out and (Unnamed Layer* 406) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 411) [ElementWise]_output and (Unnamed Layer* 407) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 413) [ElementWise]_output and (Unnamed Layer* 408) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 415) [Activation]_output and (Unnamed Layer* 409) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 416) [ElementWise]_output and (Unnamed Layer* 410) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 419) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 404) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 423) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 424) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l21_fc_mid_out and (Unnamed Layer* 426) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 431) [ElementWise]_output and (Unnamed Layer* 427) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 433) [ElementWise]_output and (Unnamed Layer* 428) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 435) [Activation]_output and (Unnamed Layer* 429) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 436) [ElementWise]_output and (Unnamed Layer* 430) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 439) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 424) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 443) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 444) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l22_fc_mid_out and (Unnamed Layer* 446) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 451) [ElementWise]_output and (Unnamed Layer* 447) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 453) [ElementWise]_output and (Unnamed Layer* 448) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 455) [Activation]_output and (Unnamed Layer* 449) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 456) [ElementWise]_output and (Unnamed Layer* 450) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 459) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 444) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [I] Using default for use_int8_scale_max: true
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 463) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 464) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs l23_fc_mid_out and (Unnamed Layer* 466) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 471) [ElementWise]_output and (Unnamed Layer* 467) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 473) [ElementWise]_output and (Unnamed Layer* 468) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 475) [Activation]_output and (Unnamed Layer* 469) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 476) [ElementWise]_output and (Unnamed Layer* 470) [Constant]_output: first input has type Half but second input has type Float.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_gelu_out. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 479) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 464) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[05/11/2024-21:13:47] [TRT] [W] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor input_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor segment_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor cu_seqlens, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor max_seqlen, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l0_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 6) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 7) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 8) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 9) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 10) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 11) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 12) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 13) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 14) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 15) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 16) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 17) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l1_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 26) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 27) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 28) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 29) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 30) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 31) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 32) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 33) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 34) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 35) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 36) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 37) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l2_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 46) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 47) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 48) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 49) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 50) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 51) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 52) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 53) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 54) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 55) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 56) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 57) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l3_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 66) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 67) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 68) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 69) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 70) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 71) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 72) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 73) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 74) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 75) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 76) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 77) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l4_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 86) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 87) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 88) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 89) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 90) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 91) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 92) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 93) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 94) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 95) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 96) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 97) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l5_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 106) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 107) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 108) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 109) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 110) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 111) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 112) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 113) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 114) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 115) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 116) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 117) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l6_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 126) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 127) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 128) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 129) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 130) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 131) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 132) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 133) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 134) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 135) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 136) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 137) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l7_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 146) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 147) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 148) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 149) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 150) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 151) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 152) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 153) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 154) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 155) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 156) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 157) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l8_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 166) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 167) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 168) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 169) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 170) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 171) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 172) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 173) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 174) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 175) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 176) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 177) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l9_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 186) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 187) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 188) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 189) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 190) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 191) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 192) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 193) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 194) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 195) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 196) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 197) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l10_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 206) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 207) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 208) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 209) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 210) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 211) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 212) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 213) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 214) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 215) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 216) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 217) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l11_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 226) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 227) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 228) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 229) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 230) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 231) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 232) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 233) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 234) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 235) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 236) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 237) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l12_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 246) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 247) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 248) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 249) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 250) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 251) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 252) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 253) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 254) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 255) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 256) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 257) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l13_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 266) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 267) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 268) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 269) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 270) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 271) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 272) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 273) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 274) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 275) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 276) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 277) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l14_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 286) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 287) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 288) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 289) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 290) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 291) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 292) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 293) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 294) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 295) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 296) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 297) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l15_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 306) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 307) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 308) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 309) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 310) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 311) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 312) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 313) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 314) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 315) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 316) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 317) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l16_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 326) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 327) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 328) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 329) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 330) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 331) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 332) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 333) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 334) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 335) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 336) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 337) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l17_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 346) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 347) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 348) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 349) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 350) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 351) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 352) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 353) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 354) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 355) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 356) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 357) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l18_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 366) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 367) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 368) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 369) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 370) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 371) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 372) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 373) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 374) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 375) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 376) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 377) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l19_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 386) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 387) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 388) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 389) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 390) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 391) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 392) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 393) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 394) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 395) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 396) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 397) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l20_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 406) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 407) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 408) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 409) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 410) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 411) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 412) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 413) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 414) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 415) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 416) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 417) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l21_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 426) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 427) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 428) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 429) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 430) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 431) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 432) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 433) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 434) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 435) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 436) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 437) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l22_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 446) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 447) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 448) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 449) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 450) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 451) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 452) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 453) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 454) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 455) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 456) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 457) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor l23_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 466) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 467) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 468) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 469) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 470) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 471) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 472) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 473) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 474) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 475) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 476) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:47] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 477) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/11/2024-21:13:49] [TRT] [I] Graph optimization time: 1.58839 seconds.
[05/11/2024-21:13:49] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +8, now: CPU 3814, GPU 958 (MiB)
[05/11/2024-21:13:49] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 3815, GPU 968 (MiB)
[05/11/2024-21:13:49] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 30000 detected for tactic 0xf067e6205da31c2e.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 1 due to insufficient memory on requested size of 30000 detected for tactic 0x503619c69ae500ff.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 2 due to insufficient memory on requested size of 30000 detected for tactic 0xa8ef60e712f8ad24.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 3 due to insufficient memory on requested size of 30000 detected for tactic 0x3f243c490d502deb.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 4 due to insufficient memory on requested size of 30000 detected for tactic 0x9808072e706def96.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 5 due to insufficient memory on requested size of 30000 detected for tactic 0xa8609adc4e0ceb90.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 6 due to insufficient memory on requested size of 30000 detected for tactic 0xf64396b97c889179.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 7 due to insufficient memory on requested size of 30000 detected for tactic 0xc3cf6e1d1c6aff27.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020775.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 1 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002020e.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 2 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020419.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 3 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020777.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 4 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002076f.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 5 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020453.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 6 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020490.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 7 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020729.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 8 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000205f3.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 9 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020774.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 10 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002076c.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 11 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000200b3.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 12 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020592.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 13 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020442.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 14 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000200e8.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 15 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002038b.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 16 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002076b.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 17 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000203cc.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 18 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020732.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 19 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000203f3.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 20 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020771.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 21 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020623.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 22 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002054f.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 23 due to insufficient memory on requested size of 30000 detected for tactic 0x0000000000020615.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 24 due to insufficient memory on requested size of 30000 detected for tactic 0x000000000002054a.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 25 due to insufficient memory on requested size of 30000 detected for tactic 0x00000000000204e4.
[05/11/2024-21:13:49] [TRT] [I] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[05/11/2024-21:13:49] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:49] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 30000 detected for tactic 0x17173deba0b64484.
[05/11/2024-21:13:50] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:50] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 30000 detected for tactic 0x3e2b881168d9689d.
[05/11/2024-21:13:50] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:50] [TRT] [I] Skipping tactic 1 due to insufficient memory on requested size of 30000 detected for tactic 0xe47307053a42b3e4.
[05/11/2024-21:13:50] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:50] [TRT] [I] Skipping tactic 2 due to insufficient memory on requested size of 30000 detected for tactic 0xf90060ce8193b811.
[05/11/2024-21:13:50] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:50] [TRT] [I] Skipping tactic 3 due to insufficient memory on requested size of 30000 detected for tactic 0x7bc32c782b800c48.
[05/11/2024-21:13:50] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:50] [TRT] [I] Skipping tactic 4 due to insufficient memory on requested size of 30000 detected for tactic 0xc7feb33970feefa7.
[05/11/2024-21:13:50] [TRT] [W] Tactic Device request: 30000MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:13:50] [TRT] [I] Skipping tactic 5 due to insufficient memory on requested size of 30000 detected for tactic 0xae0c89d047932ba3.
[05/11/2024-21:16:02] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:02] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 24375 detected for tactic 0x69c4e2ca38eadce2.
[05/11/2024-21:16:02] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:02] [TRT] [I] Skipping tactic 1 due to insufficient memory on requested size of 24375 detected for tactic 0xff6944b17d5b2e32.
[05/11/2024-21:16:02] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:02] [TRT] [I] Skipping tactic 2 due to insufficient memory on requested size of 24375 detected for tactic 0xa4ae2d82115c3e83.
[05/11/2024-21:16:02] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:02] [TRT] [I] Skipping tactic 3 due to insufficient memory on requested size of 24375 detected for tactic 0xfe80445f7bb61a99.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 0 due to insufficient memory on requested size of 24375 detected for tactic 0x2468d082d0ff7c9a.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 1 due to insufficient memory on requested size of 24375 detected for tactic 0x9003f5f7ff9b1aec.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 2 due to insufficient memory on requested size of 24375 detected for tactic 0x271b998fe31732ef.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 3 due to insufficient memory on requested size of 24375 detected for tactic 0xcd229658c16b33cd.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 4 due to insufficient memory on requested size of 24375 detected for tactic 0xa2f15b0a75b7dcec.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 5 due to insufficient memory on requested size of 24375 detected for tactic 0x91930a570b557437.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 6 due to insufficient memory on requested size of 24375 detected for tactic 0x445983715412fbda.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 7 due to insufficient memory on requested size of 24375 detected for tactic 0x991db9fd58152c33.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 8 due to insufficient memory on requested size of 24375 detected for tactic 0x84942841d92b0552.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 9 due to insufficient memory on requested size of 24375 detected for tactic 0x844ea9c00f711f19.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 10 due to insufficient memory on requested size of 24375 detected for tactic 0x1e55f8b415964e81.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 11 due to insufficient memory on requested size of 24375 detected for tactic 0xdc1f355deb032b87.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 12 due to insufficient memory on requested size of 24375 detected for tactic 0x5f1a472d416ff35e.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 13 due to insufficient memory on requested size of 24375 detected for tactic 0xb5f710b331ba8377.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 14 due to insufficient memory on requested size of 24375 detected for tactic 0x748f926574b7bdc4.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 15 due to insufficient memory on requested size of 24375 detected for tactic 0xb86b920539eb9c79.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 16 due to insufficient memory on requested size of 24375 detected for tactic 0xdf7e1bd6a496d667.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 17 due to insufficient memory on requested size of 24375 detected for tactic 0xbeb5d91e1874a437.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 18 due to insufficient memory on requested size of 24375 detected for tactic 0x5bd8221bd57baf93.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 19 due to insufficient memory on requested size of 24375 detected for tactic 0x8141573686849b61.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 20 due to insufficient memory on requested size of 24375 detected for tactic 0x6d377e4222886190.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 21 due to insufficient memory on requested size of 24375 detected for tactic 0x9ec201b34455146e.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 22 due to insufficient memory on requested size of 24375 detected for tactic 0x960e9baa2a6cad5b.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 23 due to insufficient memory on requested size of 24375 detected for tactic 0x7720f198395e7d3d.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 24 due to insufficient memory on requested size of 24375 detected for tactic 0x33a5c6dd086942c1.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 25 due to insufficient memory on requested size of 24375 detected for tactic 0x65fbe45b4cb1d8a5.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 26 due to insufficient memory on requested size of 24375 detected for tactic 0x60b880e28fee7a0c.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 27 due to insufficient memory on requested size of 24375 detected for tactic 0x6986b0c6a136276e.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 28 due to insufficient memory on requested size of 24375 detected for tactic 0xcf64a2ae51bf6b36.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 29 due to insufficient memory on requested size of 24375 detected for tactic 0xe742f4598442d2f1.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 30 due to insufficient memory on requested size of 24375 detected for tactic 0xa71946688cad8664.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 31 due to insufficient memory on requested size of 24375 detected for tactic 0x3f2b14dec582741e.
[05/11/2024-21:16:23] [TRT] [W] Tactic Device request: 24375MB Available: 24061MB. Device memory is insufficient to use tactic.
[05/11/2024-21:16:23] [TRT] [I] Skipping tactic 32 due to insufficient memory on requested size of 24375 detected for tactic 0x6e9c17a33c93d9b0.
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0xcba01f45d37a76a0 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x264d83735b1cba13 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x8e1dd2962c589dd4 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x7de8ad674a85e82a due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0xaba07efc49c11a6c due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0xd1a1d0d7a29590b7 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0xaceaca2c82ab40f3 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x27fb8661fea664b3 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x43e3804f158e597d due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x682fae8be946411d due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x4e7f02f91ddf4673 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0xaa30815cbd0f74b7 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x8f07802f58d278c0 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x758f8b2079a95b2e due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x68f52f0a3f1c5b56 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x6c336d1b7c64ffa3 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x86d1488f426124e1 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x5812ced7874b44d1 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0xb7a90e7097d64877 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x8486adb55ae0ca6c due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x62a338704beec449 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0xb34706d47a62a016 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x3e116db8858d0d41 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0xcf392eca056b8d88 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x70ffb5fe2e1321e3 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0xdb3ae212a7704d44 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0xd7985d83133bcf37 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x7524377e24bc511f due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x4f8662a723b489e1 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0x1cfa820c55616892 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [I] Skipping tactic 0xccdb99df0646c7b3 due to exception an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_qkv : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [W] GPU error during getBestTactic: l0_fc_aout : an illegal memory access was encountered
[05/11/2024-21:16:24] [TRT] [E] /_src/plugin/common/bertCommon.h (300) - Cuda Error in operator(): 700 (an illegal memory access was encountered)
make[1]: Leaving directory '/work'
